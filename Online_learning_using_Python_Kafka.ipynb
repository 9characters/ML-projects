{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Online learning using Python Kafka",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9characters/ML-projects/blob/main/Online_learning_using_Python_Kafka.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vFCn-xQwg0Gh"
      },
      "source": [
        "# Lab 6: Online Learning using Apache Kafka\n",
        "#### Using Apache Kafka to simulate real time data stream model training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18H_dm9etzO6"
      },
      "source": [
        "### Learning Objecives\n",
        "* Learn to import kafka and write to and read from kafka.\n",
        "* Learn to use layers in keras.\n",
        "* Learn to use kafka for data streams and train the model in real-time.\n",
        "* Learn the concept of online learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZhPKWnzhR_w"
      },
      "source": [
        "### Install Package\n",
        "Install Kafka and Tensorflow IO. Tensorflow IO is an extension package to Tensorflow which supports integration with Apache Kafka and other systems. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "39jwbs8spzhJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "878f47c5-73eb-42d8-9e36-f9ff76f07f42"
      },
      "source": [
        "!pip install tensorflow-io\n",
        "!pip install kafka-python"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorflow-io\n",
            "  Downloading tensorflow_io-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (22.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 22.7 MB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorflow<2.7.0,>=2.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-io) (2.6.0)\n",
            "Collecting tensorflow-io-gcs-filesystem==0.21.0\n",
            "  Downloading tensorflow_io_gcs_filesystem-0.21.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1 MB 48.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.2)\n",
            "Requirement already satisfied: numpy~=1.19.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.19.5)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.0)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.6.3)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.0)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.15.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.12.0)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.0)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.37.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.17.3)\n",
            "Requirement already satisfied: keras~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12)\n",
            "Requirement already satisfied: tensorflow-estimator~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.37.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.41.0)\n",
            "Requirement already satisfied: clang~=5.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (5.0)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.7.4.3)\n",
            "Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.6.0)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.12.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.5.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.8.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.6.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.23.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.7.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (4.8.1)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2021.5.30)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tensorboard~=2.6->tensorflow<2.7.0,>=2.6.0->tensorflow-io) (3.6.0)\n",
            "Installing collected packages: tensorflow-io-gcs-filesystem, tensorflow-io\n",
            "Successfully installed tensorflow-io-0.21.0 tensorflow-io-gcs-filesystem-0.21.0\n",
            "Collecting kafka-python\n",
            "  Downloading kafka_python-2.0.2-py2.py3-none-any.whl (246 kB)\n",
            "\u001b[K     |████████████████████████████████| 246 kB 11.5 MB/s \n",
            "\u001b[?25hInstalling collected packages: kafka-python\n",
            "Successfully installed kafka-python-2.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E14eihOmhdlZ"
      },
      "source": [
        "### Imports\n",
        "Import all the required libraries for the lab including Kafka, pandas, Tensorflow, and more.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CFgauMIEsb8b"
      },
      "source": [
        "import time \n",
        "from kafka import KafkaProducer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd \n",
        "import tensorflow as tf\n",
        "import tensorflow_io as tfio"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ybkg2_UiwLQ"
      },
      "source": [
        "### Download Kafka\n",
        "Download and setup Kafka for real time data stream simulation. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HB7oqVR7xlsm"
      },
      "source": [
        "!curl -sSOL https://downloads.apache.org/kafka/2.8.1/kafka_2.12-2.8.1.tgz\n",
        "!tar -xzf kafka_2.12-2.8.1.tgz "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fV6xtkye4WBV"
      },
      "source": [
        "### Start running Kafka and Zookeeper server instances \n",
        "\n",
        "Start Kafka and Zookeeper servers as a daemon processes. Zookeeper is a centralized service for maintaing configuration information, naming, providing distributed synchronization, and providing group services."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMc545ia_C9N"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/zookeeper-server-start.sh -daemon ./kafka_2.12-2.8.1/config/zookeeper.properties\n",
        "!./kafka_2.12-2.8.1/bin/kafka-server-start.sh -daemon ./kafka_2.12-2.8.1/config/server.properties"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6F_VBN2I7f0V"
      },
      "source": [
        "### Create a topic to store events\n",
        "Create topic for train and test dataset to store events in Kafka. Kafka is a distributed event streaming platform that lets you read, write, store and process events. These events or messages are organized and stored in topics. In simple terms, topic is similar to a folder in a filesystem, and the events are the file in that folder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8EBXCVYAG70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c1277a-9a5c-4e2a-ec23-4ee384578b11"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-train --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1\n",
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --create --topic susy-test --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2021-10-29 18:56:42,847] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
            "[2021-10-29 18:56:42,891] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
            "[2021-10-29 18:56:43,093] WARN [AdminClient clientId=adminclient-1] Connection to node -1 (localhost/127.0.0.1:9092) could not be established. Broker may not be available. (org.apache.kafka.clients.NetworkClient)\n",
            "Created topic susy-train.\n",
            "Created topic susy-test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bx5BZYklEDiI"
      },
      "source": [
        "### Describe the topic for details\n",
        "Describe command helps us gather details on topic, it's partitions, replicas, and other important information.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXLs0nkvDaUG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b92bf530-3373-4ca8-d3a5-da543094b233"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-train --bootstrap-server localhost:9092\n",
        "!./kafka_2.12-2.8.1/bin/kafka-topics.sh --describe --topic susy-test --bootstrap-server localhost:9092"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: susy-train\tTopicId: _iexn9tNT4Kg7WQ5JjaDlQ\tPartitionCount: 1\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-train\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "Topic: susy-test\tTopicId: g4xrW9ZqSLi8Mm0yYf0Hug\tPartitionCount: 2\tReplicationFactor: 1\tConfigs: segment.bytes=1073741824\n",
            "\tTopic: susy-test\tPartition: 0\tLeader: 0\tReplicas: 0\tIsr: 0\n",
            "\tTopic: susy-test\tPartition: 1\tLeader: 0\tReplicas: 0\tIsr: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-NJOBi6Ea-6"
      },
      "source": [
        "### Mount Google Drive\n",
        "\n",
        "In the code cell below, we mount the google drive to the colab environment so that we have access to the local version of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9oSRp6vUqV_",
        "outputId": "9b693826-8c72-4fa5-c35b-bd53ce5b8321"
      },
      "source": [
        "!gdown --id \"1y0Ej7oYbKKB6YdK_yJ1OkCv6V_a7JAgk\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1y0Ej7oYbKKB6YdK_yJ1OkCv6V_a7JAgk\n",
            "To: /content/SUSY.csv\n",
            "100% 2.39G/2.39G [00:25<00:00, 93.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36sU7_q2EgTj"
      },
      "source": [
        "### Define features\n",
        "COLUMNS is used to define each of the feature in the SUSY dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mMIPL9cKh_1"
      },
      "source": [
        "COLUMNS = [\n",
        "           'class',\n",
        "           'lepton_1_pT',\n",
        "           'lepton_1_eta',\n",
        "           'lepton_1_phi',\n",
        "           'lepton_2_pT',\n",
        "           'lepton_2_eta',\n",
        "           'lepton_2_phi',\n",
        "           'missing_energy_magnitude',\n",
        "           'missing_energy_phi',\n",
        "           'MET_rel',\n",
        "           'axial_MET',\n",
        "           'M_R',\n",
        "           'M_TR_2',\n",
        "           'R',\n",
        "           'MT2',\n",
        "           'S_R',\n",
        "           'M_Delta_R',\n",
        "           'dPhi_r_b',\n",
        "           'cos(theta_r1)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUbBS3M_FYx8"
      },
      "source": [
        "### SUSY Dataset\n",
        "\n",
        "SUSY Data set is produced using Monte Carlo simulations. It is the data produced from the particle accelerators. The first column of the dataset is the label followed by 8 features which are kinematic properties measured by the particle detectors in the accelerator. The last 10 features are the high-level features derived by physicists to help discriminate between the two classes signal process or a background process.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtjgFSdAEyuK"
      },
      "source": [
        "### Read CSV\n",
        "Use Pandas to load SUSY dataset from the CSV file and provide the column name for each of the features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ6F6wlsJCMJ"
      },
      "source": [
        "mydata = pd.read_csv('SUSY.csv', header=None, names=COLUMNS, nrows=100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a7PBfl_AFJBH"
      },
      "source": [
        "### Visualize Data\n",
        "\n",
        "Panda functions helps us visualize the SUSY dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqi_w6yhKNT2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "d241bb74-773a-40ac-804b-d8717b880bac"
      },
      "source": [
        "mydata.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>lepton_1_pT</th>\n",
              "      <th>lepton_1_eta</th>\n",
              "      <th>lepton_1_phi</th>\n",
              "      <th>lepton_2_pT</th>\n",
              "      <th>lepton_2_eta</th>\n",
              "      <th>lepton_2_phi</th>\n",
              "      <th>missing_energy_magnitude</th>\n",
              "      <th>missing_energy_phi</th>\n",
              "      <th>MET_rel</th>\n",
              "      <th>axial_MET</th>\n",
              "      <th>M_R</th>\n",
              "      <th>M_TR_2</th>\n",
              "      <th>R</th>\n",
              "      <th>MT2</th>\n",
              "      <th>S_R</th>\n",
              "      <th>M_Delta_R</th>\n",
              "      <th>dPhi_r_b</th>\n",
              "      <th>cos(theta_r1)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.972861</td>\n",
              "      <td>0.653855</td>\n",
              "      <td>1.176225</td>\n",
              "      <td>1.157156</td>\n",
              "      <td>-1.739873</td>\n",
              "      <td>-0.874309</td>\n",
              "      <td>0.567765</td>\n",
              "      <td>-0.175000</td>\n",
              "      <td>0.810061</td>\n",
              "      <td>-0.252552</td>\n",
              "      <td>1.921887</td>\n",
              "      <td>0.889637</td>\n",
              "      <td>0.410772</td>\n",
              "      <td>1.145621</td>\n",
              "      <td>1.932632</td>\n",
              "      <td>0.994464</td>\n",
              "      <td>1.367815</td>\n",
              "      <td>0.040714</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.667973</td>\n",
              "      <td>0.064191</td>\n",
              "      <td>-1.225171</td>\n",
              "      <td>0.506102</td>\n",
              "      <td>-0.338939</td>\n",
              "      <td>1.672543</td>\n",
              "      <td>3.475464</td>\n",
              "      <td>-1.219136</td>\n",
              "      <td>0.012955</td>\n",
              "      <td>3.775174</td>\n",
              "      <td>1.045977</td>\n",
              "      <td>0.568051</td>\n",
              "      <td>0.481928</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.448410</td>\n",
              "      <td>0.205356</td>\n",
              "      <td>1.321893</td>\n",
              "      <td>0.377584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.444840</td>\n",
              "      <td>-0.134298</td>\n",
              "      <td>-0.709972</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>-1.613871</td>\n",
              "      <td>-0.768661</td>\n",
              "      <td>1.219918</td>\n",
              "      <td>0.504026</td>\n",
              "      <td>1.831248</td>\n",
              "      <td>-0.431385</td>\n",
              "      <td>0.526283</td>\n",
              "      <td>0.941514</td>\n",
              "      <td>1.587535</td>\n",
              "      <td>2.024308</td>\n",
              "      <td>0.603498</td>\n",
              "      <td>1.562374</td>\n",
              "      <td>1.135454</td>\n",
              "      <td>0.180910</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.381256</td>\n",
              "      <td>-0.976145</td>\n",
              "      <td>0.693152</td>\n",
              "      <td>0.448959</td>\n",
              "      <td>0.891753</td>\n",
              "      <td>-0.677328</td>\n",
              "      <td>2.033060</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>3.046260</td>\n",
              "      <td>-1.005285</td>\n",
              "      <td>0.569386</td>\n",
              "      <td>1.015211</td>\n",
              "      <td>1.582217</td>\n",
              "      <td>1.551914</td>\n",
              "      <td>0.761215</td>\n",
              "      <td>1.715464</td>\n",
              "      <td>1.492257</td>\n",
              "      <td>0.090719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>1.309996</td>\n",
              "      <td>-0.690089</td>\n",
              "      <td>-0.676259</td>\n",
              "      <td>1.589283</td>\n",
              "      <td>-0.693326</td>\n",
              "      <td>0.622907</td>\n",
              "      <td>1.087562</td>\n",
              "      <td>-0.381742</td>\n",
              "      <td>0.589204</td>\n",
              "      <td>1.365479</td>\n",
              "      <td>1.179295</td>\n",
              "      <td>0.968218</td>\n",
              "      <td>0.728563</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.083158</td>\n",
              "      <td>0.043429</td>\n",
              "      <td>1.154854</td>\n",
              "      <td>0.094859</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   class  lepton_1_pT  lepton_1_eta  ...  M_Delta_R  dPhi_r_b  cos(theta_r1)\n",
              "0    0.0     0.972861      0.653855  ...   0.994464  1.367815       0.040714\n",
              "1    1.0     1.667973      0.064191  ...   0.205356  1.321893       0.377584\n",
              "2    1.0     0.444840     -0.134298  ...   1.562374  1.135454       0.180910\n",
              "3    1.0     0.381256     -0.976145  ...   1.715464  1.492257       0.090719\n",
              "4    1.0     1.309996     -0.690089  ...   0.043429  1.154854       0.094859\n",
              "\n",
              "[5 rows x 19 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrX5owSuGmFo"
      },
      "source": [
        "### Split Train and Test data\n",
        "As always, it is necessary to split the data into train, test, and validation. In this context, we are splitting 80% of the data to be the train data. The remaining 20% of the data is split between test and validation dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lizEmDGHMO4j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "538292b1-d3a2-49a1-97a6-efeaacdeaf03"
      },
      "source": [
        "train_df, test_df = train_test_split(mydata, test_size=0.3, shuffle=True)\n",
        "print(\"Number of training samples: \",len(train_df))\n",
        "print(\"Number of testing sample: \",len(test_df))\n",
        "\n",
        "x_train_df = train_df.drop([\"class\"], axis=1)\n",
        "y_train_df = train_df[\"class\"]\n",
        "\n",
        "x_test_df = test_df.drop([\"class\"], axis=1)\n",
        "y_test_df = test_df[\"class\"]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples:  70000\n",
            "Number of testing sample:  30000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xs_7J7ViHJJw"
      },
      "source": [
        "### Convert data to list format\n",
        "Read each row from the dataframe and convert it to the list format to feed to Kafka."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "36wbZHBmTWcM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94b668e5-9053-460e-8e06-17969c277994"
      },
      "source": [
        "#Convert each test and train dataframe to list form to feed to kafka\n",
        "x_train = list(filter(None, x_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_train = list(filter(None, y_train_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "\n",
        "x_test = list(filter(None, x_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "y_test = list(filter(None, y_test_df.to_csv(index=False).split(\"\\n\")[1:]))\n",
        "len(x_train), len(y_train), len(x_test), len(y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(70000, 70000, 30000, 30000)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MR9ptIMmaLNH"
      },
      "source": [
        "NUM_COLUMNS = len(x_train_df.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdX-5m7CJZZl"
      },
      "source": [
        "### Create Kafka Producer \n",
        "Create Kafka producer which takes in data and sends the record to the partition within a topic in Kafka cluster. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JVGpokJzWDHO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d09d6c8-3d48-4026-e8d9-4b221c07cae2"
      },
      "source": [
        "#send each record to a partition within a topic in kafka cluster\n",
        "def write_to_kafka(topic, items):\n",
        "  count=0\n",
        "  producer = KafkaProducer(bootstrap_servers=['localhost:9092'])\n",
        "  for message, key in items:\n",
        "    producer.send(topic, key=key.encode('utf-8'), value=message.encode('utf-8'))\n",
        "    count += 1 \n",
        "  producer.flush()\n",
        "  print(\"Wrote {0} messages into topic: {1}\".format(count, topic))\n",
        "\n",
        "write_to_kafka(\"susy-train\", zip(x_train, y_train))\n",
        "write_to_kafka(\"susy-test\", zip(x_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wrote 70000 messages into topic: susy-train\n",
            "Wrote 30000 messages into topic: susy-test\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJcUt38yKwrn"
      },
      "source": [
        "### Online Learning\n",
        "Unlike traditional training of machine learning models, online learning is based on incrementally learning or updating parameters as soon as the new data points are available. This process continues indefinitely. In the code below, stream_timeout is set to 10000 milliseconds which means as all the messages are consumed from the topic, the dataset will wait for 10 more seconds before timing out and disconnecting from the Kafka cluster. If additional data arrives in that time period, model training resumes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tritmkkgwv9x"
      },
      "source": [
        "online_train_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-train\"],\n",
        "    group_id=\"cgonline\",\n",
        "    servers=\"localhost:9092\",\n",
        "    stream_timeout=10000, # in milliseconds, to block indefinitely, set it to -1.\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_online_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "online_train_ds = online_train_ds.shuffle(buffer_size=32)\n",
        "online_train_ds = online_train_ds.map(decode_kafka_online_item)\n",
        "online_train_ds = online_train_ds.batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5VFMGf9J6QG"
      },
      "source": [
        "### Initialize variables to create ANN\n",
        "Initialize Optimizer, loss function, metrics function, and epochs for Neural Network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOSkIPwgEOYE"
      },
      "source": [
        "OPTIMIZER=\"adam\"\n",
        "LOSS = tf.keras.losses.BinaryCrossentropy()\n",
        "METRICS = ['accuracy']\n",
        "EPOCHS = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5f_oGoOKXEI"
      },
      "source": [
        "### Define Model\n",
        "Create Neural network containing multiple layers and use Dropout layer to prevent overfitting in model. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SY8OZdXdXJ26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb9929c2-f12c-4358-ed0b-9959d375afdf"
      },
      "source": [
        "#define model input shape, and layers of NN\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(128, input_shape=(NUM_COLUMNS,), activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.2))\n",
        "model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_8 (Dense)              (None, 128)               2432      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 68,481\n",
            "Trainable params: 68,481\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUIOEbm5Zxx1"
      },
      "source": [
        "model.compile(optimizer=OPTIMIZER, loss=LOSS, metrics=METRICS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9NmCVdIeSG5"
      },
      "source": [
        "model.fit(online_train_ds, epochs=EPOCHS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUxQs72-MMGn"
      },
      "source": [
        "### Prepare test data\n",
        "\n",
        "Prepare the test dataset using KafkaGroupIODataset to stream and test with the model we initialized before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMLaZdqte2ak"
      },
      "source": [
        "#prepare test data to evaluate on the online trained model\n",
        "test_ds = tfio.experimental.streaming.KafkaGroupIODataset(\n",
        "    topics=[\"susy-test\"],\n",
        "    group_id=\"testcg\",\n",
        "    servers=\"localhost:9092\",\n",
        "    stream_timeout=10000,\n",
        "    configuration=[\n",
        "        \"session.timeout.ms=7000\",\n",
        "        \"max.poll.interval.ms=8000\",\n",
        "        \"auto.offset.reset=earliest\"\n",
        "    ],\n",
        ")\n",
        "\n",
        "def decode_kafka_test_item(raw_message, raw_key):\n",
        "  message = tf.io.decode_csv(raw_message, [[0.0] for i in range(NUM_COLUMNS)])\n",
        "  key = tf.strings.to_number(raw_key)\n",
        "  return (message, key)\n",
        "\n",
        "test_ds = test_ds.map(decode_kafka_test_item)\n",
        "test_ds = test_ds.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EDZ9PN8SARD"
      },
      "source": [
        "### Evaluate Model\n",
        "Use predefined function evaluate to figure loss value and metric value with the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aj2rb9p3ylS4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76d27283-a835-4d95-cfb4-e70b770f15b9"
      },
      "source": [
        "res = model.evaluate(test_ds)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 23s 24ms/step - loss: 0.4459 - accuracy: 0.7925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thlar0_-783K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940c5c68-f7b8-4201-d046-8a8f0d41aea8"
      },
      "source": [
        "!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group cgonline\n",
        "!./kafka_2.12-2.8.1/bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group testcg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Consumer group 'cgonline' has no active members.\n",
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
            "cgonline        susy-train      0          70000           70000           0               -               -               -\n",
            "\n",
            "Consumer group 'testcg' has no active members.\n",
            "\n",
            "GROUP           TOPIC           PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             CONSUMER-ID     HOST            CLIENT-ID\n",
            "testcg          susy-test       0          16223           16223           0               -               -               -\n",
            "testcg          susy-test       1          13777           13777           0               -               -               -\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "slTfvrP0lPix"
      },
      "source": [
        "**What is the difference between this Lab (Lab 6) and Lab 4 (IDS)? [5 points]**\n",
        "\n",
        "In Lab 4, we do not incorporate the change in dataset or any incremental learning. The data was static and just trained over the data. But in Lab 6, we continually get the data as the real-time data steram. This will help to learn the new data continually using the tools like apache-kafka in lab 6.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vkRNwArNErT0"
      },
      "source": [
        "**Did you observe any differences in result during the evaluation of the model when you rerun? [5 points]**\n",
        "\n",
        "When we try to run the model again with the data from the same topic \"sysy-train\", then the error \"Empty array\" appears. This means that the data from the topic has already been used up, and there is no new data in th event stream.\n",
        "\n",
        "Still, if we try to run the same program starting from the first, then we see that the accuracy and loss have not changed significantly. Therefore, to see the changes in the evaluations metrices, more topics with new data need to be created and trained using the Neural Networks Model. The results on multiple runs are as follows:\n",
        "\n",
        "First Run => loss: 0.4421 - accuracy: 0.7940\n",
        "\n",
        "Second Run => loss: 0.4407 - accuracy: 0.7971\n",
        "\n",
        "Third Run => loss: 0.4459 - accuracy: 0.7925\n",
        "\n"
      ]
    }
  ]
}